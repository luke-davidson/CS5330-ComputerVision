{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6a5c0151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from numpy.core.fromnumeric import shape\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "def Dataset(typeInput):\n",
    "    src = '/Users/lukedavidson/Downloads/pa4/datasets/DevanagariHandwrittenCharacterDataset/Test'\n",
    "    allFileNames = os.listdir(src)\n",
    "    character_list = []\n",
    "    digit_list = []\n",
    "    for list_name in allFileNames:\n",
    "        if list_name.startswith(\"character_\"):\n",
    "            character_list.append(list_name)#[10:])\n",
    "        elif (list_name.startswith(\"digit_\")):\n",
    "            digit_list.append(list_name)#[6:])\n",
    "\n",
    "    count = 0\n",
    "    label_num = 0\n",
    "    \n",
    "    if typeInput == 'Test':\n",
    "        output_data = np.empty((10800,3,32,32))\n",
    "    elif typeInput == 'Train':\n",
    "        output_data = np.empty((61200,3,32,32))\n",
    "    else:\n",
    "        raise NotImplementedError('Invalid typeInput.')\n",
    "        \n",
    "    output_labels = np.array([])\n",
    "    \n",
    "    \n",
    "    for i in character_list:\n",
    "        files = glob.glob('/Users/lukedavidson/Downloads/pa4/datasets/DevanagariHandwrittenCharacterDataset/'+typeInput+'/'+i+'/*.png')\n",
    "        for myFile in files:\n",
    "#             print(myFile)\n",
    "            image = cv2.imread(myFile)\n",
    "            image = np.transpose(image, axes = (2,0,1))\n",
    "            if count == 0:\n",
    "                output_data[0,:,:,:] = np.asarray(image)\n",
    "                output_labels = np.array([label_num])\n",
    "                count += 1\n",
    "            else:\n",
    "                output_data[count,:,:,:] = np.asarray(image)\n",
    "                output_labels = np.append(output_labels, label_num)\n",
    "                count += 1\n",
    "#         print('{} is label #{}'.format(i,label_num))\n",
    "        label_num += 1\n",
    "    print('{} data shape: {}'.format(typeInput, output_data.shape))\n",
    "    print('{} label shape: {}\\n'.format(typeInput, output_labels.shape))\n",
    "    return output_data, output_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ec43d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_val_subset_data(input_data, input_labels):\n",
    "    \n",
    "    random_index = np.random.randint(0,10800,1000)\n",
    "    subset_data = input_data[random_index,:,:,:]\n",
    "    subset_labels = input_labels[random_index]\n",
    "    \n",
    "    return subset_data, subset_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "66fc7707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (61200, 3, 32, 32)\n",
      "Train label shape: (61200,)\n",
      "\n",
      "Test data shape: (1000, 3, 32, 32)\n",
      "Test label shape: (1000,)\n",
      "\n",
      "Validation data shape: (1000, 3, 32, 32)\n",
      "Validation label shape: (1000,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize Train data\n",
    "x_train_data, y_train_data = Dataset('Train')\n",
    "\n",
    "# initialize Test data\n",
    "x_test_data, y_test_data = test_val_subset_data(x_train_data, y_train_data)\n",
    "print('Test data shape: {}'.format(x_test_data.shape))\n",
    "print('Test label shape: {}\\n'.format(y_test_data.shape))\n",
    "\n",
    "# initialize Validation data\n",
    "x_val_data, y_val_data = test_val_subset_data(x_train_data, y_train_data)\n",
    "print('Validation data shape: {}'.format(x_val_data.shape))\n",
    "print('Validation label shape: {}\\n'.format(y_val_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6298b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNet(object):\n",
    "    \"\"\"\n",
    "    A fully-connected neural network with an arbitrary number of hidden layers,\n",
    "    ReLU nonlinearities, and a softmax loss function. For a network with L layers,\n",
    "    the architecture will be\n",
    "\n",
    "    {affine - relu} x (L - 1) - affine - softmax\n",
    "\n",
    "    where batch/layer normalization and dropout are optional, and the {...} block is\n",
    "    repeated L - 1 times.\n",
    "\n",
    "    Similar to the TwoLayerNet above, learnable parameters are stored in the\n",
    "    self.params dictionary and will be learned using the Solver class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dims, input_dim=3*32*32, num_classes=10, reg=0.0,\n",
    "                 weight_scale=1e-2, dtype=np.float32, seed=None):\n",
    "        \"\"\"\n",
    "        Initialize a new FullyConnectedNet.\n",
    "\n",
    "        Inputs:\n",
    "        - hidden_dims: A list of integers giving the size of each hidden layer.\n",
    "        - input_dim: An integer giving the size of the input.\n",
    "        - num_classes: An integer giving the number of classes to classify.\n",
    "        - reg: Scalar giving L2 regularization strength.\n",
    "        - weight_scale: Scalar giving the standard deviation for random\n",
    "          initialization of the weights.\n",
    "        - dtype: A numpy datatype object; all computations will be performed using\n",
    "          this datatype. float32 is faster but less accurate, so you should use\n",
    "          float64 for numeric gradient checking.\n",
    "        - seed: If not None, then pass this random seed to the dropout layers. This\n",
    "          will make the dropout layers deteriminstic so we can gradient check the\n",
    "          model.\n",
    "        \"\"\"\n",
    "        self.reg = reg\n",
    "        self.num_layers = 1 + len(hidden_dims)\n",
    "        self.dtype = dtype\n",
    "        self.params = {}\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Initialize the parameters of the network, storing all values in    #\n",
    "        # the self.params dictionary. Store weights and biases for the first layer #\n",
    "        # in W1 and b1; for the second layer use W2 and b2, etc. Weights should be #\n",
    "        # initialized from a normal distribution centered at 0 with standard       #\n",
    "        # deviation equal to weight_scale. Biases should be initialized to zero.   #\n",
    "        #                                                                          #\n",
    "        # When using batch normalization, store scale and shift parameters for the #\n",
    "        # first layer in gamma1 and beta1; for the second layer use gamma2 and     #\n",
    "        # beta2, etc. Scale parameters should be initialized to ones and shift     #\n",
    "        # parameters should be initialized to zeros.                               #\n",
    "        ############################################################################\n",
    "#         # appended list dims\n",
    "#         input_dim = list([input_dim])\n",
    "#         hidden_dims = list(hidden_dims)\n",
    "#         num_classes = list([num_classes])\n",
    "        \n",
    "#         print('input dim: {}'.format(input_dim))\n",
    "        print('hidden dims: {}'.format(hidden_dims))\n",
    "#         print('num classes: {}'.format(num_classes))\n",
    "#         print('num_layers: {}'.format(self.num_layers))\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            W_layer = 'W' + str(i+1)\n",
    "            b_layer = 'b' + str(i+1)\n",
    "            if i == 0:    #\n",
    "                self.params[W_layer] = np.random.normal(0.0,weight_scale,(input_dim,hidden_dims[0]))\n",
    "                self.params[b_layer] = np.zeros(hidden_dims[0])\n",
    "            elif i+1 == self.num_layers:    #\n",
    "                self.params[W_layer] = np.random.normal(0.0,weight_scale,(hidden_dims[len(hidden_dims)-1],num_classes))\n",
    "                self.params[b_layer] = np.zeros(num_classes)\n",
    "            else:    #\n",
    "                self.params[W_layer] = np.random.normal(0.0,weight_scale,(hidden_dims[i-1],hidden_dims[i]))\n",
    "                self.params[b_layer] = np.zeros(hidden_dims[i])\n",
    "        \n",
    "#         raise NotImplementedError\n",
    "    \n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "        # Cast all parameters to the correct datatype\n",
    "        for k, v in self.params.items():\n",
    "            self.params[k] = v.astype(dtype)\n",
    "\n",
    "\n",
    "    def loss(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Compute loss and gradient for the fully-connected net.\n",
    "\n",
    "        Input / output: Same as TwoLayerNet above.\n",
    "        \"\"\"\n",
    "        X = X.astype(self.dtype)\n",
    "        mode = 'test' if y is None else 'train'\n",
    "\n",
    "        scores = None\n",
    "        self.cache = {}\n",
    "        ############################################################################\n",
    "        # TODO: Implement the forward pass for the fully-connected net, computing  #\n",
    "        # the class scores for X and storing them in the scores variable.          #\n",
    "        #                                                                          #\n",
    "        # When using dropout, you'll need to pass self.dropout_param to each       #\n",
    "        # dropout forward pass.                                                    #\n",
    "        #                                                                          #\n",
    "        # When using batch normalization, you'll need to pass self.bn_params[0] to #\n",
    "        # the forward pass for the first batch normalization layer, pass           #\n",
    "        # self.bn_params[1] to the forward pass for the second batch normalization #\n",
    "        # layer, etc.                                                              #\n",
    "        ############################################################################\n",
    "        \n",
    "        # initialize score variable as the same as X, then update\n",
    "        scores = X\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            W_layer = 'W' + str(i+1)\n",
    "            b_layer = 'b' + str(i+1)\n",
    "            cache_layer = 'cache' + str(i+1)\n",
    "#             print('scores {} shape: {}'.format(i+1,scores.shape))\n",
    "            if i+1 == self.num_layers:\n",
    "                scores, cache = affine_forward(scores,self.params[W_layer],self.params[b_layer])\n",
    "            else:\n",
    "                scores, cache = affine_relu_forward(scores, self.params[W_layer], self.params[b_layer])\n",
    "            self.cache[cache_layer] = cache\n",
    "#             print('W{} shape: {}'.format(i+1,self.params[W_layer].shape))\n",
    "#             print('X shape: {}'.format(X.shape))\n",
    "            \n",
    "#         raise NotImplementedError\n",
    "\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "        # If test mode return early\n",
    "        if mode == 'test':\n",
    "            return scores\n",
    "\n",
    "        loss, grads = 0.0, {}\n",
    "        ############################################################################\n",
    "        # TODO: Implement the backward pass for the fully-connected net. Store the #\n",
    "        # loss in the loss variable and gradients in the grads dictionary. Compute #\n",
    "        # data loss using softmax, and make sure that grads[k] holds the gradients #\n",
    "        # for self.params[k]. Don't forget to add L2 regularization!               #\n",
    "        #                                                                          #\n",
    "        # When using batch/layer normalization, you don't need to regularize the scale   #\n",
    "        # and shift parameters.                                                    #\n",
    "        #                                                                          #\n",
    "        # NOTE: To ensure that your implementation matches ours and you pass the   #\n",
    "        # automated tests, make sure that your L2 regularization includes a factor #\n",
    "        # of 0.5 to simplify the expression for the gradient.                      #\n",
    "        ############################################################################\n",
    "        \n",
    "        loss, dx = softmax_loss(scores, y)\n",
    "        \n",
    "#         print('scores shape: {}'.format(scores.shape))\n",
    "#         print('dx shape: {}'.format(dx.shape))\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            W_layer = 'W' + str(i+1)\n",
    "            new_reg = 0.5*self.reg*np.sum(self.params[W_layer]**2)\n",
    "            loss += new_reg\n",
    "    \n",
    "        for i in reversed(range(self.num_layers)):\n",
    "            W_layer = 'W' + str(i+1)\n",
    "            b_layer = 'b' + str(i+1)\n",
    "            cache_layer = 'cache' + str(i+1)\n",
    "            if i+1 == self.num_layers:    #grads[W] = dw, grads[b] = db\n",
    "                dx, grads[W_layer], grads[b_layer] = affine_backward(dx,self.cache[cache_layer])\n",
    "            else:\n",
    "                dx, grads[W_layer], grads[b_layer] = affine_relu_backward(dx,self.cache[cache_layer])\n",
    "            grads[W_layer] += (self.reg)*(self.params[W_layer])\n",
    "            \n",
    "#         raise NotImplementedError\n",
    "    \n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "        return loss, grads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f810870",
   "metadata": {},
   "source": [
    "   #            Handwritten Character Recognition for Devanagari Script\n",
    "                                Luke Davidson and Siddhant Ashay Shah\n",
    "\n",
    "    In this notebook, we have built both a fully-connected network and Convolutional Neural network and adjusted their builds and parameters to achieve the best validation accuracy possible on a set of handwritten Devanagari alphabetical characters.  By creating a fully-connected network using the ReLU activation function and softmax loss classifier, we were able to alter the build of the network, including number of hidden layers, batche sizes of training and validation data, and specific parameters such as the weight scale of the weights and learning rate to achieve desired results on the UCI Machine Learning Repository: Devanagari Handwritten Character Dataset Data Set.  Similarly, by altering the architectural build of our CNN, we were able to work with minibatches of our data to simulate potential large scale applications of a similar fully-connected network, prefaced by convolution layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b290b",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d01eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.solver import Solver\n",
    "import os\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from numpy.core.fromnumeric import shape\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2764ecc9",
   "metadata": {},
   "source": [
    "# Import and Define the data sets\n",
    "We have created two functions here that read and separate sets of handwritten samples of Devanagari script.  The first function, Dataset(typeInput), reads the files from the downloaded dataset (either 'Test' or 'Train'), converts them from .pgm pictures to numpy arrays of size (N,3,32,32), and assigns correct labels for them.  Please uncomment the line \"print('{} is label #{}'.format(i,label_num))\" to see which characters correspond to each label.\n",
    "The second function, random_subset_data(input_data, input_labels, num_subsamples), creates a certain number of subsamples of the data, num_subsamples, to divide in to Test and Validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e58b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dataset(typeInput):\n",
    "    '''\n",
    "    typeInput = 'Test' or 'Train'; and reads the files from the given folder and converts them to numpy arrays of \n",
    "        size (32,32).\n",
    "    src = directory to the downloaded data set. Ex. '/Users/lukedavidson/Downloads/pa4/datasets/DevanagariHandwrittenCharacterDataset/Test'\n",
    "    \n",
    "    Outputs:\n",
    "    output_data: output data of shape (N,3,32,32)\n",
    "    output_labels: corresponding labels of the data of shape (N,). Integers ranging from 0-35.\n",
    "    '''\n",
    "    \n",
    "    # input path to downloaded folder \"DevanagariHandwrittenCharacterDataset/Test\" below:\n",
    "    src = '/Users/lukedavidson/Downloads/pa4/datasets/DevanagariHandwrittenCharacterDataset/Test'\n",
    "    allFileNames = os.listdir(src)\n",
    "    character_list = []\n",
    "    digit_list = []\n",
    "    for list_name in allFileNames:\n",
    "        if list_name.startswith(\"character_\"):\n",
    "            character_list.append(list_name)#[10:])\n",
    "        elif (list_name.startswith(\"digit_\")):\n",
    "            digit_list.append(list_name)#[6:])\n",
    "\n",
    "    count = 0\n",
    "    label_num = 0\n",
    "    \n",
    "    if typeInput == 'Test':\n",
    "        output_data = np.empty((10800,3,32,32))\n",
    "    elif typeInput == 'Train':\n",
    "        output_data = np.empty((61200,3,32,32))\n",
    "    else:\n",
    "        raise NotImplementedError('Invalid typeInput.')\n",
    "        \n",
    "    output_labels = np.array([])\n",
    "    \n",
    "    for i in character_list:    # put path to \"DevanagariHandwrittenCharacterDataset\" here in this format\n",
    "        files = glob.glob('/Users/lukedavidson/Downloads/pa4/datasets/DevanagariHandwrittenCharacterDataset/'+typeInput+'/'+i+'/*.png')\n",
    "        for myFile in files:\n",
    "            image = cv2.imread(myFile)\n",
    "            image = np.transpose(image, axes = (2,0,1))\n",
    "            if count == 0:\n",
    "                output_data[0,:,:,:] = np.asarray(image)\n",
    "                output_labels = np.array([label_num])\n",
    "                count += 1\n",
    "            else:\n",
    "                output_data[count,:,:,:] = np.asarray(image)\n",
    "                output_labels = np.append(output_labels, label_num)\n",
    "                count += 1\n",
    "#         print('{} is label #{}'.format(i,label_num))\n",
    "        label_num += 1\n",
    "    print('{} data shape: {}'.format(typeInput, output_data.shape))\n",
    "    print('{} label shape: {}\\n'.format(typeInput, output_labels.shape))\n",
    "    return output_data, output_labels\n",
    "\n",
    "def random_subset_data(input_data, input_labels, num_subsamples):\n",
    "    \n",
    "    random_index = np.random.randint(0,10800,num_subsamples)\n",
    "    subset_data = input_data[random_index,:,:,:]\n",
    "    subset_labels = input_labels[random_index]\n",
    "    \n",
    "    return subset_data, subset_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fc3fb8",
   "metadata": {},
   "source": [
    "# Initialization of all training, test, and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67736fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (61200, 3, 32, 32)\n",
      "Train label shape: (61200,)\n",
      "\n",
      "Test data shape: (10800, 3, 32, 32)\n",
      "Test label shape: (10800,)\n",
      "\n",
      "Test sub data shape: (1000, 3, 32, 32)\n",
      "Test sub label shape: (1000,)\n",
      "\n",
      "Validation data shape: (1000, 3, 32, 32)\n",
      "Validation label shape: (1000,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize Train data\n",
    "x_train_data, y_train_data = Dataset('Train')\n",
    "x_full_test_data, y_full_test_data = Dataset('Test')\n",
    "\n",
    "# initialize Test data\n",
    "x_test_data, y_test_data = random_subset_data(x_full_test_data, y_full_test_data,1000)\n",
    "print('Test sub data shape: {}'.format(x_test_data.shape))\n",
    "print('Test sub label shape: {}\\n'.format(y_test_data.shape))\n",
    "\n",
    "# initialize Validation data\n",
    "x_val_data, y_val_data = random_subset_data(x_full_test_data, y_full_test_data,1000)\n",
    "print('Validation data shape: {}'.format(x_val_data.shape))\n",
    "print('Validation label shape: {}\\n'.format(y_val_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c416416",
   "metadata": {},
   "source": [
    "# Beginning of function definitions for fully-connected net\n",
    "Below is where we have defined all the functions involved with our fully-connected net, including affine layers, ReLU activation function layers, softmax loss classification, and sgd momentum calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5b186ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    \n",
    "    x_reshape = x.reshape(x.shape[0],w.shape[0])\n",
    "    out = x_reshape @ w + b\n",
    "    cache = (x, w, b)\n",
    "    \n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7ebcd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_backward(dout, cache):\n",
    "\n",
    "    x, w, b = cache\n",
    "    dx = (dout @ w.T).reshape(x.shape)\n",
    "    x_reshape = x.reshape(x.shape[0],w.shape[0])\n",
    "    dw = x_reshape.T @ dout\n",
    "    db = np.sum(dout, axis=0)\n",
    "\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3645659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    \n",
    "    out = x\n",
    "    cache = x\n",
    "    out[x<=0] = 0\n",
    "\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f529459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "\n",
    "    x = cache\n",
    "    dx = dout\n",
    "    dx[x<=0] = 0\n",
    "\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75fbcf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_relu_forward(x, w, b):\n",
    "\n",
    "    x_affine, cache_affine = affine_forward(x,w,b)\n",
    "    out, cache_relu = relu_forward(x_affine)\n",
    "    cache = (cache_affine, cache_relu)\n",
    "\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "896a4cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_relu_backward(dout, cache):\n",
    "\n",
    "    cache_affine, cache_relu = cache\n",
    "    d_in = relu_backward(dout, cache_relu)\n",
    "    dx, dw, db = affine_backward(d_in, cache_affine)\n",
    "\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ade5cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(x, y):\n",
    "\n",
    "    prob = np.exp(x)    #e^ of subsample vs scores\n",
    "    prob_row_sum = np.sum(prob,axis=1)    #sum of each e^ row for each subsample\n",
    "    x_inc = np.arange(x.shape[0])    #initialize x-increments\n",
    "    numerator = prob[x_inc,y]    #numerator array for syi\n",
    "    Li = -np.log(numerator/prob_row_sum)    #Loss array for each subsample\n",
    "    loss = np.sum(Li)/Li.shape[0]    #total loss / N\n",
    "    dx = prob/prob_row_sum.reshape(x.shape[0],1)    #e^x / row sum for dx calc\n",
    "    dx[x_inc,y] -= 1    #setting correct vals\n",
    "    dx /= x.shape[0]    #dx / N\n",
    "\n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e222eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_momentum(w, dw, config=None):\n",
    "\n",
    "    if config is None: config = {}\n",
    "    config.setdefault('learning_rate', 1e-2)\n",
    "    config.setdefault('momentum', 0.9)\n",
    "    v = config.get('velocity', np.zeros_like(w))\n",
    "    next_v = config['momentum']*v - config['learning_rate']*dw\n",
    "    next_w = w + next_v\n",
    "    config['velocity'] = next_v\n",
    "\n",
    "    return next_w, config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dd99b8",
   "metadata": {},
   "source": [
    "# Building of the fully-connected network\n",
    "Here is where we have built the architecture for our fully-connected network.  Inputs include:\n",
    "\n",
    "\n",
    "Number of hidden dimensions: a list of hidden dimensions, in which the length of the list defines the number of iterations the fully-connected network completes.  \n",
    "\n",
    "Input dimensions of the data: In our case, this will be 3x32x32 for each image.  \n",
    "\n",
    "Number of classes of the data: In our case, this will be 36, representing the number of characters in the Devanagari script.  \n",
    "\n",
    "Regularization strength: Desired regularization strength when normalizing the data.  \n",
    "\n",
    "A weight scale: weight scale applied to the weights between each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ba4fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNet(object):\n",
    "\n",
    "    def __init__(self, hidden_dims, input_dim=3*32*32, num_classes=36, reg=0.0,\n",
    "                 weight_scale=1e-2, dtype=np.float32, seed=None):\n",
    "\n",
    "        self.reg = reg\n",
    "        self.num_layers = 1 + len(hidden_dims)\n",
    "        self.dtype = dtype\n",
    "        self.params = {}\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            W_layer = 'W' + str(i+1)\n",
    "            b_layer = 'b' + str(i+1)\n",
    "            if i == 0:    #\n",
    "                self.params[W_layer] = np.random.normal(0.0,weight_scale,(input_dim,hidden_dims[0]))\n",
    "                self.params[b_layer] = np.zeros(hidden_dims[0])\n",
    "            elif i+1 == self.num_layers:    #\n",
    "                self.params[W_layer] = np.random.normal(0.0,weight_scale,(hidden_dims[len(hidden_dims)-1],num_classes))\n",
    "                self.params[b_layer] = np.zeros(num_classes)\n",
    "            else:    #\n",
    "                self.params[W_layer] = np.random.normal(0.0,weight_scale,(hidden_dims[i-1],hidden_dims[i]))\n",
    "                self.params[b_layer] = np.zeros(hidden_dims[i])\n",
    "        \n",
    "        for k, v in self.params.items():\n",
    "            self.params[k] = v.astype(dtype)\n",
    "\n",
    "\n",
    "    def loss(self, X, y=None):\n",
    "\n",
    "        X = X.astype(self.dtype)\n",
    "        mode = 'test' if y is None else 'train'\n",
    "\n",
    "        scores = None\n",
    "        self.cache = {}\n",
    "\n",
    "        scores = X\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            W_layer = 'W' + str(i+1)\n",
    "            b_layer = 'b' + str(i+1)\n",
    "            cache_layer = 'cache' + str(i+1)\n",
    "            if i+1 == self.num_layers:\n",
    "                scores, cache = affine_forward(scores,self.params[W_layer],self.params[b_layer])\n",
    "            else:\n",
    "                scores, cache = affine_relu_forward(scores, self.params[W_layer], self.params[b_layer])\n",
    "            self.cache[cache_layer] = cache\n",
    "\n",
    "        if mode == 'test':\n",
    "            return scores\n",
    "\n",
    "        loss, grads = 0.0, {}\n",
    "\n",
    "        loss, dx = softmax_loss(scores, y)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            W_layer = 'W' + str(i+1)\n",
    "            new_reg = 0.5*self.reg*np.sum(self.params[W_layer]**2)\n",
    "            loss += new_reg\n",
    "    \n",
    "        for i in reversed(range(self.num_layers)):\n",
    "            W_layer = 'W' + str(i+1)\n",
    "            b_layer = 'b' + str(i+1)\n",
    "            cache_layer = 'cache' + str(i+1)\n",
    "            if i+1 == self.num_layers:    #grads[W] = dw, grads[b] = db\n",
    "                dx, grads[W_layer], grads[b_layer] = affine_backward(dx,self.cache[cache_layer])\n",
    "            else:\n",
    "                dx, grads[W_layer], grads[b_layer] = affine_relu_backward(dx,self.cache[cache_layer])\n",
    "            grads[W_layer] += (self.reg)*(self.params[W_layer])\n",
    "\n",
    "        return loss, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551c7251",
   "metadata": {},
   "source": [
    "# Training the fully-connected network\n",
    "\n",
    "We have created a fully-connected model in which we can tune the parameters and architecture of the whole model.  Using the solver from PA4, we can print out both training and validation accuracies, and compare the best results on our datasets with the individual parameters of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b97e5ae3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 12200) loss: 11.660968\n",
      "(Epoch 0 / 50) train acc: 0.039000; val_acc: 0.029000\n",
      "(Epoch 1 / 50) train acc: 0.552000; val_acc: 0.542000\n",
      "(Iteration 251 / 12200) loss: 3.005475\n",
      "(Epoch 2 / 50) train acc: 0.644000; val_acc: 0.656000\n",
      "(Iteration 501 / 12200) loss: 1.763948\n",
      "(Epoch 3 / 50) train acc: 0.560000; val_acc: 0.540000\n",
      "(Iteration 751 / 12200) loss: 1.906674\n",
      "(Epoch 4 / 50) train acc: 0.711000; val_acc: 0.728000\n",
      "(Iteration 1001 / 12200) loss: 1.900547\n",
      "(Epoch 5 / 50) train acc: 0.781000; val_acc: 0.779000\n",
      "(Iteration 1251 / 12200) loss: 1.659061\n",
      "(Epoch 6 / 50) train acc: 0.780000; val_acc: 0.752000\n",
      "(Iteration 1501 / 12200) loss: 1.583443\n",
      "(Epoch 7 / 50) train acc: 0.759000; val_acc: 0.737000\n",
      "(Iteration 1751 / 12200) loss: 1.621375\n",
      "(Epoch 8 / 50) train acc: 0.785000; val_acc: 0.787000\n",
      "(Iteration 2001 / 12200) loss: 1.534966\n",
      "(Epoch 9 / 50) train acc: 0.781000; val_acc: 0.788000\n",
      "(Iteration 2251 / 12200) loss: 1.734004\n",
      "(Epoch 10 / 50) train acc: 0.811000; val_acc: 0.789000\n",
      "(Iteration 2501 / 12200) loss: 1.545588\n",
      "(Epoch 11 / 50) train acc: 0.811000; val_acc: 0.804000\n",
      "(Iteration 2751 / 12200) loss: 1.721069\n",
      "(Epoch 12 / 50) train acc: 0.809000; val_acc: 0.819000\n",
      "(Iteration 3001 / 12200) loss: 1.519461\n",
      "(Epoch 13 / 50) train acc: 0.822000; val_acc: 0.824000\n",
      "(Iteration 3251 / 12200) loss: 1.613211\n",
      "(Epoch 14 / 50) train acc: 0.773000; val_acc: 0.744000\n",
      "(Iteration 3501 / 12200) loss: 1.462645\n",
      "(Epoch 15 / 50) train acc: 0.802000; val_acc: 0.795000\n",
      "(Iteration 3751 / 12200) loss: 1.425661\n",
      "(Epoch 16 / 50) train acc: 0.830000; val_acc: 0.809000\n",
      "(Iteration 4001 / 12200) loss: 1.384381\n",
      "(Epoch 17 / 50) train acc: 0.825000; val_acc: 0.816000\n",
      "(Iteration 4251 / 12200) loss: 1.484970\n",
      "(Epoch 18 / 50) train acc: 0.828000; val_acc: 0.822000\n",
      "(Iteration 4501 / 12200) loss: 1.341897\n",
      "(Epoch 19 / 50) train acc: 0.764000; val_acc: 0.750000\n",
      "(Iteration 4751 / 12200) loss: 1.471973\n",
      "(Epoch 20 / 50) train acc: 0.825000; val_acc: 0.819000\n",
      "(Iteration 5001 / 12200) loss: 1.493880\n",
      "(Epoch 21 / 50) train acc: 0.836000; val_acc: 0.831000\n",
      "(Iteration 5251 / 12200) loss: 1.492403\n",
      "(Epoch 22 / 50) train acc: 0.827000; val_acc: 0.816000\n",
      "(Iteration 5501 / 12200) loss: 1.351363\n",
      "(Epoch 23 / 50) train acc: 0.804000; val_acc: 0.797000\n",
      "(Iteration 5751 / 12200) loss: 1.391177\n",
      "(Epoch 24 / 50) train acc: 0.835000; val_acc: 0.827000\n",
      "(Iteration 6001 / 12200) loss: 1.384184\n",
      "(Epoch 25 / 50) train acc: 0.865000; val_acc: 0.844000\n",
      "(Iteration 6251 / 12200) loss: 1.524134\n",
      "(Epoch 26 / 50) train acc: 0.826000; val_acc: 0.815000\n",
      "(Iteration 6501 / 12200) loss: 1.333329\n",
      "(Epoch 27 / 50) train acc: 0.834000; val_acc: 0.813000\n",
      "(Iteration 6751 / 12200) loss: 1.422252\n",
      "(Epoch 28 / 50) train acc: 0.847000; val_acc: 0.839000\n",
      "(Iteration 7001 / 12200) loss: 1.291461\n",
      "(Epoch 29 / 50) train acc: 0.855000; val_acc: 0.834000\n",
      "(Iteration 7251 / 12200) loss: 1.339257\n",
      "(Epoch 30 / 50) train acc: 0.846000; val_acc: 0.843000\n",
      "(Iteration 7501 / 12200) loss: 1.277403\n",
      "(Epoch 31 / 50) train acc: 0.858000; val_acc: 0.842000\n",
      "(Iteration 7751 / 12200) loss: 1.351215\n",
      "(Epoch 32 / 50) train acc: 0.846000; val_acc: 0.836000\n",
      "(Iteration 8001 / 12200) loss: 1.307299\n",
      "(Epoch 33 / 50) train acc: 0.857000; val_acc: 0.848000\n",
      "(Iteration 8251 / 12200) loss: 1.316650\n",
      "(Epoch 34 / 50) train acc: 0.856000; val_acc: 0.846000\n",
      "(Iteration 8501 / 12200) loss: 1.438084\n",
      "(Epoch 35 / 50) train acc: 0.847000; val_acc: 0.844000\n",
      "(Iteration 8751 / 12200) loss: 1.337666\n",
      "(Epoch 36 / 50) train acc: 0.851000; val_acc: 0.844000\n",
      "(Iteration 9001 / 12200) loss: 1.368017\n",
      "(Epoch 37 / 50) train acc: 0.839000; val_acc: 0.826000\n",
      "(Iteration 9251 / 12200) loss: 1.349698\n",
      "(Epoch 38 / 50) train acc: 0.852000; val_acc: 0.843000\n",
      "(Iteration 9501 / 12200) loss: 1.304126\n",
      "(Epoch 39 / 50) train acc: 0.862000; val_acc: 0.837000\n",
      "(Iteration 9751 / 12200) loss: 1.358233\n",
      "(Epoch 40 / 50) train acc: 0.873000; val_acc: 0.840000\n",
      "(Iteration 10001 / 12200) loss: 1.277838\n",
      "(Epoch 41 / 50) train acc: 0.872000; val_acc: 0.849000\n",
      "(Epoch 42 / 50) train acc: 0.861000; val_acc: 0.848000\n",
      "(Iteration 10251 / 12200) loss: 1.499315\n",
      "(Epoch 43 / 50) train acc: 0.873000; val_acc: 0.850000\n",
      "(Iteration 10501 / 12200) loss: 1.285493\n",
      "(Epoch 44 / 50) train acc: 0.883000; val_acc: 0.844000\n",
      "(Iteration 10751 / 12200) loss: 1.356012\n",
      "(Epoch 45 / 50) train acc: 0.885000; val_acc: 0.851000\n",
      "(Iteration 11001 / 12200) loss: 1.291896\n",
      "(Epoch 46 / 50) train acc: 0.859000; val_acc: 0.846000\n",
      "(Iteration 11251 / 12200) loss: 1.301822\n",
      "(Epoch 47 / 50) train acc: 0.863000; val_acc: 0.847000\n",
      "(Iteration 11501 / 12200) loss: 1.281924\n",
      "(Epoch 48 / 50) train acc: 0.850000; val_acc: 0.848000\n",
      "(Iteration 11751 / 12200) loss: 1.295585\n",
      "(Epoch 49 / 50) train acc: 0.872000; val_acc: 0.841000\n",
      "(Iteration 12001 / 12200) loss: 1.255780\n",
      "(Epoch 50 / 50) train acc: 0.880000; val_acc: 0.850000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number layers: 2\n",
      "Hidden dimensions: [100,100]\n",
      "Weight scale: 0.01\n",
      "Learning rate: 0.009\n",
      "Batch size: 250\n"
     ]
    }
   ],
   "source": [
    "data = {'X_train': x_train_data, 'y_train': y_train_data, 'X_val': x_val_data, 'y_val': y_val_data,}\n",
    "\n",
    "model = FullyConnectedNet(hidden_dims=[100,100],weight_scale=1e-2,dtype=np.float64, reg=0.5)\n",
    "solver = Solver(model, data,\n",
    "                optim_config={\n",
    "                  'learning_rate': 9e-3,\n",
    "                },\n",
    "                lr_decay=0.96,\n",
    "                num_epochs=50, batch_size=250,\n",
    "                print_every=250)\n",
    "solver.train()\n",
    "best_model = model\n",
    "\n",
    "plt.plot(solver.loss_history, 'c')\n",
    "plt.title('Davidson, Shah - Training Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Softmax Loss')\n",
    "plt.show()\n",
    "print('Number layers: 2')\n",
    "print('Hidden dimensions: [100,100]')\n",
    "print('Weight scale: 0.01')\n",
    "print('Learning rate: 0.009')\n",
    "print('Batch size: 250')\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5344f3",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "Below is where we have created a fully-connected network that also includes layers of convolution before the affine layers and softmax loss classification.  We have completed similar steps as the fully-connected network above, however have changed the architecture of the network to match a more classic CNN build."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2893144",
   "metadata": {},
   "source": [
    "# Beginning of function definitions not included from fully-connected net\n",
    "Below are a list of the functions needed for a convolutional neural network that are not included in the fully-connected network above.  We have followed the build of a convolution --> normalization --> ReLU activation structure, followed by a fully-connected network classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8debc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_forward(x, w, b, conv_param):\n",
    "    \n",
    "    out = None\n",
    "\n",
    "    N,C,H,W = x.shape\n",
    "    F,C,fil_height,fil_width = w.shape\n",
    "    stride = conv_param['stride']\n",
    "    pad_val = conv_param['pad']\n",
    "    \n",
    "    new_H = H + 2*pad_val\n",
    "    new_W = W + 2*pad_val\n",
    "    x_new = np.empty((N,C,new_H,new_W))\n",
    "    \n",
    "    # padding\n",
    "    x_new = np.pad(x, [(0,),(0,),(pad_val,),(pad_val,)])\n",
    "\n",
    "    H_prime = int(1 + (x.shape[2] + (2*pad_val) - w.shape[2])/stride)\n",
    "    W_prime = int(1 + (x.shape[3] + (2*pad_val) - w.shape[3])/stride)\n",
    "\n",
    "    out = np.empty((N,F,H_prime,W_prime))\n",
    "    \n",
    "    for im_num in range(N):\n",
    "        for filter_num in range(F):\n",
    "            for y_inc in range(H_prime):\n",
    "                y_index_start = y_inc*stride\n",
    "                y_index_end = y_index_start + fil_height\n",
    "                for x_inc in range(W_prime):\n",
    "                    x_index_start = x_inc*stride\n",
    "                    x_index_end = x_index_start + fil_width\n",
    "                    Wx_sum = np.sum(x_new[im_num,:,y_index_start:y_index_end,x_index_start:x_index_end] * w[filter_num,:,:,:])\n",
    "                    bias = b[filter_num,]\n",
    "                    out[im_num,filter_num,y_inc,x_inc] = Wx_sum + bias    # W@x + b\n",
    "\n",
    "    cache = (x, w, b, conv_param)\n",
    "    \n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92600435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_backward(dout, cache):\n",
    "\n",
    "    dx, dw, db = None, None, None\n",
    "    x, W, b, conv_param = cache\n",
    "    \n",
    "    Nx,Cx,Hx,Wx = x.shape\n",
    "    Fw,Cw,HHw,WWw = W.shape\n",
    "    Nd,Fd,Hd,Wd = dout.shape\n",
    "    stride = conv_param['stride']\n",
    "    pad = conv_param['pad']\n",
    "    \n",
    "    # initialize matrices\n",
    "    dx = np.zeros(x.shape)                           \n",
    "    dw = np.zeros(W.shape)\n",
    "    db = np.empty((Fd,))\n",
    "    \n",
    "    for i in range(Fd):\n",
    "        db[i,] = np.sum(dout[:,i,:,:])\n",
    "    \n",
    "    # pad\n",
    "    x_new = np.pad(x,((0,),(0,),(pad,),(pad,)))\n",
    "    dx_new = np.pad(dx,((0,),(0,),(pad,),(pad,)))\n",
    "    \n",
    "    for i in range(Nd):\n",
    "        x_sub_new = x_new[i,:,:,:]\n",
    "        dx_sub_new = dx_new[i,:,:,:]\n",
    "        for j in range(Hd):\n",
    "            if stride == 1:\n",
    "                y_index_start = j*stride\n",
    "                y_index_end = y_index_start + HHw\n",
    "            else:\n",
    "                y_index_start = j*stride\n",
    "                y_index_end = y_index_start + HHw\n",
    "            for k in range(Wd):\n",
    "                if stride == 1:\n",
    "                    x_index_start = k*stride\n",
    "                    x_index_end = x_index_start + WWw\n",
    "                else:\n",
    "                    x_index_start = k*stride\n",
    "                    x_index_end = x_index_start + WWw\n",
    "                for L in range(Fd):\n",
    "                    x_sub_new_slice = x_sub_new[:,y_index_start:y_index_end,x_index_start:x_index_end]    # C,sub,sub\n",
    "                    dx_sub_new[:,y_index_start:y_index_end,x_index_start:x_index_end] += W[L,:,:,:] * dout[i, L, j, k]\n",
    "                    dw[L,:,:,:] += x_sub_new_slice * dout[i,L,j,k]\n",
    "        dx[i, :, :, :] = dx_sub_new[:, pad:-pad, pad:-pad]\n",
    "\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2caf215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_forward(x, gamma, beta, bn_param):\n",
    "\n",
    "    mode = bn_param['mode']\n",
    "    eps = bn_param.get('eps', 1e-5)\n",
    "    momentum = bn_param.get('momentum', 0.9)\n",
    "\n",
    "    N, D = x.shape\n",
    "    \n",
    "    running_mean = bn_param.get('running_mean', np.zeros(D, dtype=x.dtype))\n",
    "    running_var = bn_param.get('running_var', np.zeros(D, dtype=x.dtype))\n",
    "\n",
    "    out, cache = None, None\n",
    "    \n",
    "    if mode == 'train':\n",
    "        x_sum = np.sum(x, axis=0)\n",
    "        mean = x_sum/x.shape[0]\n",
    "        x_minus_mean = x - mean\n",
    "        x_minus_mean_sq = x_minus_mean**2\n",
    "        sigma_sq = (np.sum(x_minus_mean_sq, axis=0))/x.shape[0]\n",
    "        denom = np.sqrt(sigma_sq + eps)\n",
    "        normalized_x = x_minus_mean/denom\n",
    "        out = (normalized_x*gamma) + beta\n",
    "        running_mean = momentum * running_mean + (1 - momentum) * mean\n",
    "        running_var = momentum * running_var + (1 - momentum) * sigma_sq\n",
    "        cache = (x, gamma, x_minus_mean, denom)\n",
    "    elif mode == 'test':\n",
    "        mean = running_mean\n",
    "        sigma_sq = running_var\n",
    "        x_minus_mean = x - mean\n",
    "        denom = np.sqrt(sigma_sq+eps)\n",
    "        normalized_x = x_minus_mean/denom\n",
    "        out = normalized_x*gamma + beta\n",
    "    else:\n",
    "        raise ValueError('Invalid forward batchnorm mode \"%s\"' % mode)\n",
    "    bn_param['running_mean'] = running_mean\n",
    "    bn_param['running_var'] = running_var\n",
    "\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe1fa6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_batchnorm_forward(x, gamma, beta, bn_param):\n",
    "\n",
    "    out, cache = None, None\n",
    "    N,C,H,W = x.shape\n",
    "    x_rearrange_NHWC = np.transpose(x,axes=(0,2,3,1))\n",
    "    x_dim = N*H*W\n",
    "    x_reshape = x_rearrange_NHWC.reshape(x_dim,C)\n",
    "    out, cache = batchnorm_forward(x_reshape, gamma, beta, bn_param)\n",
    "    out_reshape = out.reshape(N,H,W,C)\n",
    "    out = np.transpose(out_reshape,axes=(0,3,1,2))\n",
    "\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d927d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_backward(dout, cache):\n",
    "\n",
    "    dx, dgamma, dbeta = None, None, None\n",
    "    x, gamma, x_minus_mean, denom = cache\n",
    "    N,D = x.shape\n",
    "    dgamma = np.sum(((x_minus_mean/denom)*dout),axis=0)\n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "    dx = (gamma/denom/N)*(N*dout-dbeta-((x_minus_mean/(denom**2))*np.sum((dout*x_minus_mean),axis=0)))\n",
    "\n",
    "    return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e674d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_batchnorm_backward(dout, cache):\n",
    "\n",
    "    dx, dgamma, dbeta = None, None, None\n",
    "    N,C,H,W = dout.shape\n",
    "    dout_rearrange_NHWC = np.transpose(dout,axes=(0,2,3,1))\n",
    "    x_dim = N*H*W\n",
    "    dout_reshape = dout_rearrange_NHWC.reshape(x_dim,C)\n",
    "    dx, dgamma, dbeta = batchnorm_backward(dout_reshape, cache)\n",
    "    dx_reshape = dx.reshape(N,H,W,C)\n",
    "    dx = np.transpose(dx_reshape,axes=(0,3,1,2))\n",
    "\n",
    "    return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b52de574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_avg_pool_forward(x):\n",
    "\n",
    "    out, cache = None, (x)\n",
    "    out = np.mean(x, axis = (2,3))\n",
    "\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29548417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_avg_pool_backward(dout, cache):\n",
    "\n",
    "    dx = None\n",
    "    x = cache\n",
    "    N,C,H,W = x.shape\n",
    "    dx = np.zeros((N,C,H,W))\n",
    "    dout_avg = dout/(H*W)\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(C):\n",
    "            dx[i,j,:,:] = dout_avg[i,j]\n",
    "\n",
    "    return dx    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0dbdfc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(object):\n",
    "\n",
    "    def __init__(self, input_dim=(3, 32, 32), num_filters=[32], filter_sizes=[7],\n",
    "            num_classes=36, weight_scale=1e-3, reg=0.0, use_batch_norm=True, \n",
    "            dtype=np.float32):\n",
    "\n",
    "        self.params = {}\n",
    "        self.reg = reg\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        assert len(num_filters) == len(filter_sizes)\n",
    "\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        \n",
    "        M = len(num_filters)\n",
    "        C,H,W = input_dim\n",
    "        \n",
    "        for i in range(M+1):\n",
    "            if i == 0:\n",
    "                F = num_filters[i]\n",
    "                HH_WW = filter_sizes[i]\n",
    "                if use_batch_norm == True:\n",
    "                    W_id = 'W' + str(i+1)\n",
    "                    b_id = 'b' + str(i+1)\n",
    "                    gamma_id = 'gamma' + str(i+1)\n",
    "                    beta_id = 'beta' + str(i+1)\n",
    "                        # W = (F,C,HH,WW)\n",
    "                    self.params[W_id] = weight_scale*np.random.randn(F,C,HH_WW,HH_WW)\n",
    "                        # b = (F,)\n",
    "                    self.params[b_id] = np.zeros(F)\n",
    "                        # gamma = (F,)\n",
    "                    self.params[gamma_id] = np.ones(F)\n",
    "                        # beta = (F,)\n",
    "                    self.params[beta_id] = np.zeros(F)\n",
    "                elif use_batch_norm == False:\n",
    "                    W_id = 'W' + str(i+1)\n",
    "                    b_id = 'b' + str(i+1)\n",
    "                    self.params[W_id] = weight_scale*np.random.randn(F,C,HH_WW,HH_WW)\n",
    "                    self.params[b_id] = np.zeros(F)\n",
    "                else:\n",
    "                    raise NotImplementedError('Invalid use_batch_norm value.')\n",
    "            elif i == M:\n",
    "                if use_batch_norm == True:\n",
    "                    W_id = 'W' + str(i+1)\n",
    "                    b_id = 'b' + str(i+1)\n",
    "                        # W = (F_last,Classes)\n",
    "                    self.params[W_id] = weight_scale*np.random.randn(F,num_classes)\n",
    "                        # b = (Classes,)\n",
    "                    self.params[b_id] = np.zeros(num_classes)\n",
    "                elif use_batch_norm == False:\n",
    "                    W_id = 'W' + str(i+1)\n",
    "                    b_id = 'b' + str(i+1)\n",
    "                    self.params[W_id] = weight_scale*np.random.randn(F,num_classes)\n",
    "                    self.params[b_id] = np.zeros(num_classes)\n",
    "                else:\n",
    "                    raise NotImplementedError('Invalid use_batch_norm value.')\n",
    "            else:\n",
    "                F = num_filters[i]\n",
    "                F_last = num_filters[i-1]\n",
    "                HH_WW = filter_sizes[i]\n",
    "                if use_batch_norm == True:\n",
    "                    W_id = 'W' + str(i+1)\n",
    "                    b_id = 'b' + str(i+1)\n",
    "                    gamma_id = 'gamma' + str(i+1)\n",
    "                    beta_id = 'beta' + str(i+1)\n",
    "                        # W = (F,F_last,HH,WW)\n",
    "                    self.params[W_id] = weight_scale*np.random.randn(F,F_last,HH_WW,HH_WW)\n",
    "                        # b = (F,)\n",
    "                    self.params[b_id] = np.zeros(F)\n",
    "                        # gamma = (F,)\n",
    "                    self.params[gamma_id] = np.ones(F)\n",
    "                        # beta = (F,)\n",
    "                    self.params[beta_id] = np.zeros(F)\n",
    "                elif use_batch_norm == False:\n",
    "                    W_id = 'W' + str(i+1)\n",
    "                    b_id = 'b' + str(i+1)\n",
    "                    self.params[W_id] = weight_scale*np.random.randn(F,F_last,HH_WW,HH_WW)\n",
    "                    self.params[b_id] = np.zeros(F)\n",
    "                else:\n",
    "                    raise NotImplementedError('Invalid use_batch_norm value.')\n",
    "\n",
    "        for k, v in self.params.items():\n",
    "            self.params[k] = v.astype(dtype)\n",
    "\n",
    "\n",
    "    def loss(self, X, y=None):\n",
    "\n",
    "        scores = None\n",
    "        mode = 'test' if y is None else 'train'\n",
    "\n",
    "        cache = []\n",
    "        \n",
    "        filter_sizes = self.filter_sizes\n",
    "        num_filters = self.num_filters\n",
    "        use_batch_norm = self.use_batch_norm\n",
    "        \n",
    "        conv_param['stride'] = 1\n",
    "        M = len(num_filters)\n",
    "        \n",
    "        out_relu = X\n",
    "        \n",
    "        if use_batch_norm == True:\n",
    "            for i in range(M):\n",
    "                conv_param['pad'] = int((filter_sizes[i]-1)/2)\n",
    "                W_id = self.params['W'+str(i+1)]\n",
    "                b_id = self.params['b'+str(i+1)]\n",
    "                gamma_id = self.params['gamma'+str(i+1)]\n",
    "                beta_id = self.params['beta'+str(i+1)]\n",
    "                # convolve\n",
    "                out_convolve, cache_convolve = convolution_forward(out_relu,W_id,b_id,conv_param)\n",
    "                # batch normalization\n",
    "                del bn_param['running_mean']\n",
    "                del bn_param['running_var']\n",
    "                out_bn, cache_bn = spatial_batchnorm_forward(out_convolve,gamma_id,beta_id,bn_param)   #need to pass one fix\n",
    "                # relu\n",
    "                out_relu, cache_relu = relu_forward(out_bn)\n",
    "                cache.append((cache_convolve,cache_bn,cache_relu))\n",
    "        else:\n",
    "            for i in range(M):\n",
    "                conv_param['pad'] = int((filter_sizes[i]-1)/2)\n",
    "                W_id = self.params['W'+str(i+1)]\n",
    "                b_id = self.params['b'+str(i+1)]\n",
    "                gamma_id = self.params['gamma'+str(i+1)]\n",
    "                beta_id = self.params['beta'+str(i+1)]\n",
    "                # convolve\n",
    "                out_convolve, cache_convolve = convolution_forward(out_relu,W_id,b_id,conv_param)\n",
    "                # relu\n",
    "                out_relu, cache_relu = relu_forward(out_convolve)\n",
    "                cache.append((cache_convolve,cache_relu))\n",
    "        \n",
    "        # adaptive_average_pooling\n",
    "        out_avgpool, cache_avgpool = adaptive_avg_pool_forward(out_relu)\n",
    "        \n",
    "        # affine\n",
    "        W_id = self.params['W'+str(M+1)]\n",
    "        b_id = self.params['b'+str(M+1)]\n",
    "        scores, cache_affine = affine_forward(out_avgpool,W_id,b_id)\n",
    "\n",
    "        if y is None:\n",
    "            return scores\n",
    "\n",
    "        loss, grads = 0, {}\n",
    "\n",
    "        reg = self.reg\n",
    "        \n",
    "        # softmax\n",
    "        loss, dout = softmax_loss(scores,y)\n",
    "        \n",
    "        # first layer\n",
    "        # affine backwards\n",
    "        W_id = 'W' + str(M+1)\n",
    "        b_id = 'b' + str(M+1)\n",
    "        dx_affine, grads[W_id], grads[b_id] = affine_backward(dout,cache_affine)\n",
    "        loss += 0.5*reg*np.sum(self.params[W_id]**2)\n",
    "        grads[W_id] += reg*self.params[W_id]\n",
    "        \n",
    "        # adaptive average backwards\n",
    "        dout_avgpool = adaptive_avg_pool_backward(dx_affine, cache_avgpool)\n",
    "\n",
    "        for i in reversed(range(M)):\n",
    "            if use_batch_norm == True:\n",
    "                W_id = 'W' + str(i+1)\n",
    "                b_id = 'b' + str(i+1)\n",
    "                gamma_id = 'gamma' + str(i+1)\n",
    "                beta_id = 'beta' + str(i+1)\n",
    "                cache_convolve,cache_bn,cache_relu = cache[i]\n",
    "                    # relu back\n",
    "                if i+1 == M:\n",
    "                    dout_conv = dout_avgpool\n",
    "                dout_relu = relu_backward(dout_conv, cache_relu)\n",
    "                    # spatial back\n",
    "                dx_spatial_back, grads[gamma_id], grads[beta_id] = spatial_batchnorm_backward(dout_relu, cache_bn)\n",
    "                    # conv back\n",
    "                dout_conv, grads[W_id], grads[b_id] = convolution_backward(dx_spatial_back, cache_convolve)\n",
    "                loss += 0.5*reg*np.sum(self.params[W_id]**2)\n",
    "                grads[W_id] += reg*self.params[W_id]\n",
    "            else:\n",
    "                W_id = 'W' + str(i+1)\n",
    "                b_id = 'b' + str(i+1)\n",
    "                cache_convolve,cache_relu = cache[i]\n",
    "                    # relu back\n",
    "                dout_relu = relu_backward(dout_avgpool, cache_relu)\n",
    "                    # conv back\n",
    "                dout_conv, grads[W_id], grads[b_id] = convolution_backward(dout_relu, cache_convolve)\n",
    "                loss += 0.5*reg*np.sum(self.params[W_id]**2)\n",
    "                grads[W_id] += reg*self.params[W_id]\n",
    "            \n",
    "        return loss, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afa7847",
   "metadata": {},
   "source": [
    "# Training the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bbff892",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-203a8e0c2e7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m )\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/pa4/pa4_code/utils/solver.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0;31m# Maybe print training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/pa4/pa4_code/utils/solver.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# Compute loss and gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-420dab91e8da>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mbeta_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beta'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;31m# convolve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0mout_convolve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_convolve\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvolution_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_relu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconv_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m                 \u001b[0;31m# batch normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mbn_param\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'running_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-e2311d596fd4>\u001b[0m in \u001b[0;36mconvolution_forward\u001b[0;34m(x, w, b, conv_param)\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0mWx_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mim_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_index_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_index_end\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_index_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_index_end\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilter_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilter_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                     \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mim_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilter_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_inc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_inc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWx_sum\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m    \u001b[0;31m# W@x + b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = {'X_train': x_train_data, 'y_train': y_train_data, 'X_val': x_val_data, 'y_val': y_val_data,}\n",
    "\n",
    "conv_param = {'stride': 1}\n",
    "bn_param = {'mode':'train',\n",
    "            'eps': 2,\n",
    "            'momentum': 0.9,\n",
    "            'running_mean': None,\n",
    "            'running_var': None}\n",
    "\n",
    "model = ConvNet(\n",
    "    num_filters=[32, 16, 8],\n",
    "    filter_sizes=[7, 5, 5],\n",
    "    weight_scale=1e-2\n",
    ")\n",
    "\n",
    "solver = Solver(\n",
    "    model, data,\n",
    "    num_epochs=3, batch_size=100,\n",
    "    optim_config={\n",
    "      'learning_rate': 9e-3,\n",
    "    },\n",
    "    verbose=True, print_every=1\n",
    ")\n",
    "solver.train()\n",
    "\n",
    "best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6545bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
